{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spell_Checker_OCR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf1qlQfDiiHo"
      },
      "source": [
        "# Installing packages and modules\n",
        "As a part of this assignment, some external packages and modules are required. Namely:\n",
        "\n",
        "\n",
        "*   Tesseract and PyTesseract\n",
        "*   Swig 3.0\n",
        "*   NLTK corpora and tokenizers\n",
        "*   JamSpell and its English language model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMSPkiGlR9ll"
      },
      "source": [
        "!apt install tesseract-ocr swig3.0\n",
        "!pip install pytesseract jamspell\n",
        "!python3 -m nltk.downloader all\n",
        "!wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz\n",
        "!tar -xvzf en.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX5a6fWqi-qZ"
      },
      "source": [
        "# Importing modules\n",
        "This notebook assumes that the following Python modules are already installed and configured:\n",
        "\n",
        "\n",
        "*   OpenCV\n",
        "*   Python Imaging Library (PIL)\n",
        "*   NumPy\n",
        "*   Natural Language Toolkit (NLTK)\n",
        "*   Imutils\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak2QhmlvGaud"
      },
      "source": [
        "import cv2\n",
        "from PIL import Image, ImageSequence\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import imutils\n",
        "import nltk\n",
        "import re\n",
        "import jamspell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZyu65hIsNRo"
      },
      "source": [
        "FILE_PATH = '/content/Assignment_1.tiff'\n",
        "ROTATE = True"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBd6TfrSkIS7"
      },
      "source": [
        "# Pre-processing the image\n",
        "The image has been pre-processed using OpenCV for the OCR task. \n",
        "1. First the image is converted from RGB to grayscale\n",
        "2. Then the text orientation of the image is identified, and the image is rotated accordingly to ensure the text is in the portrait orientation.\n",
        "3. A Gaussian blur is applied to the image\n",
        "4. The image is binarized using Otsu's binarization which computes the local threshold automatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXCFLwoZwzY"
      },
      "source": [
        "def preprocessImage(path: str, rotate=True):\n",
        "  # img from RGB to gray\n",
        "  image = cv2.imread(path)\n",
        "  image= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  if rotate:\n",
        "    # identify text orientation\n",
        "    rot_data = pytesseract.image_to_osd(image)\n",
        "    rot = re.search('(?<=Rotate: )\\d+', rot_data).group(0)\n",
        "\n",
        "    # rotate image to bring it to portrait\n",
        "    image = imutils.rotate_bound(image, float(rot))\n",
        "\n",
        "  # apply Gaussian blur\n",
        "  blur = cv2.GaussianBlur(image, (3,3), 0)\n",
        "\n",
        "  # binarize the image using Otsu's binarization\n",
        "  _, image = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "  return image"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lx_ogpUlISf"
      },
      "source": [
        "# Extracting text using OCR\n",
        "Tesseract OCR extracts the textual data from the image, and provides a set of bounding box coordinates where the text was detected (text localization).\n",
        "\n",
        "A threshold confidence level of 60% has been set so that malformed or wrongly identified text is excluded from the predictions.\n",
        "\n",
        "Moreover, spaces, punctuation, special characters, contractions and ordinals have been substituted suitably"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5zOQLWyEk_X"
      },
      "source": [
        "def extractText(image):\n",
        "  # extract text from image\n",
        "  data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "  output = []\n",
        "  for i in range(0, len(data[\"text\"])):\n",
        "    # get the confidence and bounding boxes of the text localization\n",
        "    x = data[\"left\"][i]\n",
        "    y = data[\"top\"][i]\n",
        "    w = data[\"width\"][i]\n",
        "    h = data[\"height\"][i]\n",
        "    text = data[\"text\"][i]\n",
        "    confidence = int(data[\"conf\"][i])\n",
        "\n",
        "    # confidence threshold\n",
        "    if confidence>= 60:\n",
        "      # substitution map\n",
        "      subMap = { '\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', \n",
        "          '\"': ' \" ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', \n",
        "          '?':' ? ', \"n't\": \" not\" , \"'ll\": \" will\", '*':' * ', \n",
        "          '(': ' ( ', ')': ' ) ', \"s'\": \"s '\", \"1st\": \"first\",\n",
        "          \"2nd\": \"second\", \"3rd\": \"third\", \"_\": ' _ ', \"[\":\" [ \", \"]\": \" ] \"}\n",
        "      subMap = dict((re.escape(k), v) for k, v in subMap.items()) \n",
        "      pattern = re.compile(\"|\".join(subMap.keys()))\n",
        "      text = pattern.sub(lambda m: subMap[re.escape(m.group(0))], text.strip())\n",
        "\n",
        "      if(text):\n",
        "        output.append({'text': text, 'x': x, 'y': y, 'w': w, 'h': h})\n",
        "    return output"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxtrioa3mi4d"
      },
      "source": [
        "# Named Entity Recognition (NER)\n",
        "To avoid proper nouns such as names of people, places or institutions from being passed through the spell checker and potentially being wrongly corrected, these named entities are first identified and put into a list.\n",
        "\n",
        "This is done by first tokenizing the sentences, and then for each sentence, tokenizing at the word level. If the word is identified to be a named entity, it is inserted into a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30htwUeEenzr"
      },
      "source": [
        "def namedEntityRecognition(data: dict) -> set:\n",
        "  personslist = []\n",
        "  text = \" \".join(word['text'] for word in data)\n",
        "\n",
        "  # tokenize each sentence\n",
        "  for sent in nltk.sent_tokenize(text):\n",
        "    # tokenize each word\n",
        "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        # check for named entities\n",
        "          if isinstance(chunk, nltk.tree.Tree) and chunk.label() in ['PERSON', 'GPE', 'ORGANIZATION']:\n",
        "            # insert into list\n",
        "              personslist.insert(0, (chunk.leaves()[0][0]))\n",
        "\n",
        "  # convert to set to remove duplication\n",
        "  personslist = list(set(personslist))\n",
        "  # add special characters and punctuation to avoid passing them into the spell checker\n",
        "  return set(personslist + [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', '\"', \"[\", \"]\", \"{\", \"}\", \"_\"])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17RUPRangD-"
      },
      "source": [
        "# Spell checker\n",
        "Every word in the text is split, and if any of the parts of the word intersect with the ignored words set as described above, they are excluded from the spell checking process.\n",
        "\n",
        "The other words are passed into the spell checker. If the word is incorrect, it is replaced by its closest correct value. If the corrected word is different from the original word (i.e. a spelling error was detected) it is recorded with its bounding box and corrected spelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zD8Cafcgg7f"
      },
      "source": [
        "def spellChecker(data: dict, ignorewords: set) -> list:\n",
        "  text = [word['text'] for word in data]\n",
        "  corrections = []\n",
        "\n",
        "  for i, word in enumerate(text):\n",
        "    # check if the word intersects with the ignored words set\n",
        "    if not (set(word.split()) & ignorewords):\n",
        "      # put the word through the spell checker\n",
        "      corrected = corrector.FixFragment(word)\n",
        "      text[i] = corrected\n",
        "      # if the word was incorrect\n",
        "      if (corrected != word):\n",
        "        # record the correction along with its bounding box\n",
        "        corrections.append(data[i])\n",
        "        corrections[-1]['text'] = corrected\n",
        "\n",
        "  return corrections"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzUn9er9oa21"
      },
      "source": [
        "# Annotating the image with corrections\n",
        "The image is first converted back from grayscale to RGB so that annotations of a different colour may be drawn.\n",
        "\n",
        "Then for each correction that was recorded, draw a rectangle with the coordinates of the bounding box, and write the corrected word above the box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xej4SNyGO0K1"
      },
      "source": [
        "def annotateImage(image, corrections: dict):\n",
        "  backtorgb = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "  for word in corrections:\n",
        "    x, y, w, h, text = word['x'], word['y'],word['w'], word['h'], word['text']\n",
        "    image = cv2.rectangle(backtorgb, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
        "    cv2.putText(image, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "  return image"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EmVdG08vXtb"
      },
      "source": [
        "# Driver code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUr4kxN0vZ27",
        "outputId": "f9434603-73cf-4ce8-e322-566028134078"
      },
      "source": [
        "IMAGE_PATH = []\n",
        "# Splitting multi-image TIFF files into individual images\n",
        "if (FILE_PATH.endswith('.tiff')):\n",
        "  im = Image.open(FILE_PATH)\n",
        "  for i, page in enumerate(ImageSequence.Iterator(im)):\n",
        "    path = FILE_PATH.replace('.tiff', '')\n",
        "    filepath = f'{path}_{i}.png'\n",
        "    IMAGE_PATH.append(filepath)\n",
        "    page.save(filepath)\n",
        "else:\n",
        "  IMAGE_PATH.append(FILE_PATH)\n",
        "\n",
        "for path in IMAGE_PATH:\n",
        "  print(path)\n",
        "  image = preprocessImage(path, ROTATE)\n",
        "  data = extractText(image)\n",
        "  ignorewords = namedEntityRecognition(data)\n",
        "  corrections = spellChecker(data, ignorewords)\n",
        "  annotated = annotateImage(image, corrections)\n",
        "  cv2.imwrite(path.split('.')[0] + \"_corrected.png\", annotated)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Assignment_3.png\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}